{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import sys\n",
    "sys.path.append('./sportmonks_api')\n",
    "\n",
    "import pandas as pd\n",
    "from sportmonks_calls import Fixtures\n",
    "\n",
    "# 2. API Key\n",
    "API_KEY = \"oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd\"\n",
    "\n",
    "# 3. Load saved seasons\n",
    "seasons_df = pd.read_csv(\"seasons.csv\")\n",
    "print(f\"Loaded {len(seasons_df)} seasons.\")\n",
    "\n",
    "# 4. Initialize Fixtures API\n",
    "fixtures_api = Fixtures(API_KEY)\n",
    "\n",
    "# 5. Fetch fixtures for each season\n",
    "all_fixtures = []\n",
    "\n",
    "for idx, row in seasons_df.iterrows():\n",
    "    season_id = row['season_id']\n",
    "    league_name = row['league_name']\n",
    "\n",
    "    print(f\"Fetching fixtures for Season ID {season_id} ({league_name})...\")\n",
    "\n",
    "    # ✅ Correct v3 URL\n",
    "    endpoint = f\"https://api.sportmonks.com/v3/football/fixtures?seasons={season_id}\"\n",
    "\n",
    "    # Fetch fixtures\n",
    "    success = fixtures_api.request.make_request(endpoint, paginated=True)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to fetch fixtures for season {season_id}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    fixtures = fixtures_api.request.results\n",
    "    for fixture in fixtures:\n",
    "        all_fixtures.append(fixture)\n",
    "\n",
    "# 6. Build dataframe\n",
    "fixtures_df = pd.DataFrame(all_fixtures)\n",
    "print(f\"Total fixtures fetched: {len(fixtures_df)}\")\n",
    "\n",
    "# 7. (Simple view)\n",
    "display(fixtures_df.head())\n",
    "\n",
    "# 8. (Optionally save later)\n",
    "# fixtures_df.to_csv(\"fixtures.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sportmonks_calls import Fixtures\n",
    "import os\n",
    "\n",
    "# 1. Load seasons\n",
    "seasons_df = pd.read_csv(\"seasons.csv\")\n",
    "print(f\"Loaded {len(seasons_df)} seasons.\")\n",
    "\n",
    "# 2. Initialize Fixtures API\n",
    "API_KEY = \"oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd\"\n",
    "fixtures_api = Fixtures(API_KEY)\n",
    "\n",
    "# 3. Prepare fetching\n",
    "season_ids = list(seasons_df['season_id'])\n",
    "batch_size = 5  # Can later change to 10 if stable\n",
    "\n",
    "# 4. Check if partial backup exists\n",
    "if os.path.exists(\"fixtures_partial.csv\"):\n",
    "    fixtures_df = pd.read_csv(\"fixtures_partial.csv\")\n",
    "    fetched_ids = set(fixtures_df['season_id'].unique())\n",
    "    print(f\"Resuming from {len(fetched_ids)} already fetched seasons.\")\n",
    "else:\n",
    "    fixtures_df = pd.DataFrame()\n",
    "    fetched_ids = set()\n",
    "\n",
    "all_fixtures = []\n",
    "\n",
    "# 5. Loop over batches\n",
    "for i in range(0, len(season_ids), batch_size):\n",
    "    batch = season_ids[i:i+batch_size]\n",
    "\n",
    "    # Skip batch if all seasons already fetched\n",
    "    if all(season_id in fetched_ids for season_id in batch):\n",
    "        continue\n",
    "\n",
    "    batch_str = \",\".join(map(str, batch))\n",
    "    print(f\"Fetching fixtures for seasons: {batch_str}\")\n",
    "\n",
    "    # Build endpoint manually\n",
    "    endpoint = f\"https://api.sportmonks.com/v3/football/fixtures?seasons={batch_str}\"\n",
    "\n",
    "    success = fixtures_api.request.make_request(endpoint, paginated=True)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to fetch fixtures for seasons: {batch_str}\")\n",
    "        continue\n",
    "\n",
    "    fixtures = fixtures_api.request.results\n",
    "    print(f\"Fetched {len(fixtures)} fixtures.\")\n",
    "\n",
    "    # Add batch results\n",
    "    batch_df = pd.DataFrame(fixtures)\n",
    "    batch_df['season_id'] = batch  # Add season info\n",
    "    fixtures_df = pd.concat([fixtures_df, batch_df], ignore_index=True)\n",
    "\n",
    "    # Save partial progress every batch\n",
    "    fixtures_df.to_csv(\"fixtures_partial.csv\", index=False)\n",
    "    print(f\"Checkpoint saved after batch {batch_str}!\")\n",
    "\n",
    "# 6. Final save\n",
    "fixtures_df.to_csv(\"fixtures.csv\", index=False)\n",
    "print(f\"✅ Saved all {len(fixtures_df)} fixtures to fixtures.csv!\")\n",
    "print(\"You can safely delete fixtures_partial.csv if you want after verifying.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_fetch_fixtures.ipynb\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. API Key\n",
    "API_KEY = \"oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd\"\n",
    "\n",
    "# 2. Base URL\n",
    "BASE_URL = \"https://api.sportmonks.com/v3/football/fixtures\"\n",
    "\n",
    "# 3. Initialize\n",
    "all_fixtures = []\n",
    "current_page = 1\n",
    "save_every_x_pages = 20  # <- Save every 20 pages\n",
    "output_file = \"fixtures_raw.csv\"\n",
    "\n",
    "# 4. Fetch loop\n",
    "while True:\n",
    "    print(f\"Fetching page {current_page}...\")\n",
    "    \n",
    "    params = {\n",
    "        \"api_token\": API_KEY,\n",
    "        \"page\": current_page,\n",
    "    }\n",
    "    \n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code} {response.text}\")\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    if 'data' not in data:\n",
    "        print(f\"No more data on page {current_page}. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "    fixtures = data['data']\n",
    "    all_fixtures.extend(fixtures)\n",
    "    \n",
    "    # Save every X pages to be safe\n",
    "    if current_page % save_every_x_pages == 0:\n",
    "        print(f\"Saving partial results after page {current_page}...\")\n",
    "        pd.DataFrame(all_fixtures).to_csv(output_file, index=False)\n",
    "    \n",
    "    # Pagination\n",
    "    if 'pagination' in data and data['pagination']['has_more']:\n",
    "        current_page += 1\n",
    "    else:\n",
    "        print(\"No more pages left.\")\n",
    "        break\n",
    "\n",
    "    # Sleep optional (commented for now)\n",
    "    # time.sleep(0.2)\n",
    "\n",
    "# 5. Final save\n",
    "print(f\"Saving final results with {len(all_fixtures)} fixtures...\")\n",
    "pd.DataFrame(all_fixtures).to_csv(output_file, index=False)\n",
    "\n",
    "print(\"✅ Done fetching all fixtures!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 1. Set API details\n",
    "API_KEY = \"oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd\"\n",
    "BASE_URL = \"https://api.sportmonks.com/v3/football/fixtures\"\n",
    "\n",
    "# 2. Output file\n",
    "output_file = \"fixtures_full.csv\"\n",
    "save_every_x_pages = 25  # save every 25 pages\n",
    "\n",
    "# 3. Initialize\n",
    "all_fixtures = []\n",
    "current_page = 1\n",
    "\n",
    "# 4. Test connection\n",
    "print(\"Testing API connection...\")\n",
    "first_response = requests.get(BASE_URL, params={\"api_token\": API_KEY, \"page\": 1})\n",
    "\n",
    "if first_response.status_code != 200:\n",
    "    print(f\"Error: {first_response.status_code} {first_response.text}\")\n",
    "    raise Exception(\"Can't connect to Sportmonks API.\")\n",
    "\n",
    "first_data = first_response.json()\n",
    "\n",
    "# Set estimated progress for tqdm\n",
    "estimated_total = 1000\n",
    "print(\"Connection OK. Starting fetching without fixed total pages...\")\n",
    "\n",
    "# 5. Fetch loop with progress bar\n",
    "pbar = tqdm(total=estimated_total, desc=\"Fetching Fixtures\", position=0, leave=True)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"page\": current_page,\n",
    "        }\n",
    "\n",
    "        response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code} {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if 'data' not in data:\n",
    "            print(f\"No 'data' field on page {current_page}. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        fixtures = data['data']\n",
    "        all_fixtures.extend(fixtures)\n",
    "\n",
    "        if current_page % save_every_x_pages == 0:\n",
    "            pd.DataFrame(all_fixtures).to_csv(output_file, index=False)\n",
    "            print(f\"[Auto-Save] Saved {len(all_fixtures)} fixtures so far...\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        if 'pagination' in data and data['pagination']['has_more']:\n",
    "            current_page += 1\n",
    "            time.sleep(0.5)  # polite sleep to avoid hitting limits\n",
    "        else:\n",
    "            print(\"No more pages.\")\n",
    "            break\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Final save\n",
    "pd.DataFrame(all_fixtures).to_csv(output_file, index=False)\n",
    "print(f\"All done! Total fixtures saved: {len(all_fixtures)} into {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `all_fixtures` is the list containing your fetched fixture data\n",
    "df_fixtures = pd.DataFrame(all_fixtures)\n",
    "df_fixtures.to_csv(\"fixtures_full.csv\", index=False)\n",
    "print(\"Saved fixtures to fixtures_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 1. Read the fixtures file\n",
    "fixtures_df = pd.read_csv(\"fixtures_full.csv\", low_memory=False)\n",
    "fixture_ids = fixtures_df[\"id\"].dropna().astype(int).tolist()\n",
    "\n",
    "# 2. API and output\n",
    "stats_output_file = \"fixture_stats.csv\"\n",
    "save_every_x = 500  # save every 500 stats\n",
    "stats_data = []\n",
    "includes = \"stats,localTeam,visitorTeam\"\n",
    "\n",
    "print(f\"Starting to fetch statistics for {len(fixture_ids)} fixtures...\")\n",
    "\n",
    "# 3. Progress loop\n",
    "for idx, fixture_id in enumerate(tqdm(fixture_ids, desc=\"Fetching Statistics\")):\n",
    "    try:\n",
    "        url = f\"https://api.sportmonks.com/v3/football/fixtures/{fixture_id}\"\n",
    "        params = {\n",
    "            \"api_token\": API_KEY,\n",
    "            \"include\": includes\n",
    "        }\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "\n",
    "        if response.status_code == 404:\n",
    "            continue\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"[Error] Fixture {fixture_id}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        if \"data\" not in data or \"stats\" not in data[\"data\"]:\n",
    "            continue\n",
    "\n",
    "        fixture_stats = data[\"data\"]\n",
    "        fixture_stats[\"fixture_id\"] = fixture_id  # Ensure ID is preserved\n",
    "        stats_data.append(fixture_stats)\n",
    "\n",
    "        if (idx + 1) % save_every_x == 0:\n",
    "            pd.DataFrame(stats_data).to_json(stats_output_file, orient=\"records\", lines=True)\n",
    "            print(f\"[Auto-Save] {len(stats_data)} stats saved to {stats_output_file}...\")\n",
    "\n",
    "        time.sleep(0.25)  # avoid rate limit\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Exception] Fixture {fixture_id}: {e}\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "# 4. Final save\n",
    "pd.DataFrame(stats_data).to_json(stats_output_file, orient=\"records\", lines=True)\n",
    "print(f\"✅ Done! Total stats saved: {len(stats_data)} into {stats_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "API_KEY = \"oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd\"\n",
    "BASE_URL = \"https://api.sportmonks.com/v3/football/fixtures/between\"\n",
    "INCLUDE = \"stats\"\n",
    "\n",
    "# Set date range (adjust as needed) only stats from the last 5-6 seasons\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2025, 4, 30)\n",
    "step_days = 7  # fetch one week per request\n",
    "\n",
    "output_file = \"fixture_stats_batched.csv\"\n",
    "all_fixtures = []\n",
    "\n",
    "# Loop through date ranges\n",
    "print(\"Fetching fixture stats in weekly batches...\")\n",
    "while start_date < end_date:\n",
    "    batch_end = min(start_date + timedelta(days=step_days), end_date)\n",
    "    url = f\"{BASE_URL}/{start_date.date()}/{batch_end.date()}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params={\n",
    "            \"api_token\": API_KEY,\n",
    "            \"include\": INCLUDE\n",
    "        }, timeout=30)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code} for {start_date.date()} - {batch_end.date()}\")\n",
    "        else:\n",
    "            data = response.json()\n",
    "            all_fixtures.extend(data.get(\"data\", []))\n",
    "            print(f\"Fetched: {start_date.date()} to {batch_end.date()} | Total so far: {len(all_fixtures)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {start_date.date()} -> {batch_end.date()} : {e}\")\n",
    "\n",
    "    # Respect API limits\n",
    "    time.sleep(0.5)\n",
    "    start_date += timedelta(days=step_days)\n",
    "\n",
    "# Save\n",
    "df = pd.json_normalize(all_fixtures)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✅ Saved {len(df)} fixtures with stats to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Parse starting date\n",
    "fixtures_df['starting_at'] = pd.to_datetime(fixtures_df['starting_at'], errors='coerce')\n",
    "\n",
    "# Filter for fixtures from 2018 and later\n",
    "recent_fixtures_df = fixtures_df[fixtures_df['starting_at'] >= pd.Timestamp(\"2018-01-01\")]\n",
    "\n",
    "# Save or inspect filtered fixture IDs\n",
    "recent_fixture_ids = recent_fixtures_df['id'].astype(int).tolist()\n",
    "print(f\"Filtered to {len(recent_fixture_ids)} fixtures from 2018 onwards.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fixtures_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read first few lines to inspect raw structure\n",
    "with open(\"fixtures_full.csv\", \"r\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly load all columns\n",
    "fixtures_df = pd.read_csv(\"fixtures_full.csv\", low_memory=False)\n",
    "\n",
    "# Show all columns to confirm\n",
    "print(fixtures_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixtures_df['starting_at'] = pd.to_datetime(fixtures_df['starting_at'], errors='coerce')\n",
    "recent_fixtures_df = fixtures_df[fixtures_df['starting_at'] >= pd.Timestamp(\"2018-01-01\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full fixture dataset\n",
    "fixtures_df = pd.read_csv(\"/Users/sebastianvinther/Desktop/Sportsmonks/fixtures_full.csv\", low_memory=False)\n",
    "\n",
    "# Convert to datetime and drop rows with invalid dates\n",
    "fixtures_df['starting_at'] = pd.to_datetime(fixtures_df['starting_at'], errors='coerce')\n",
    "filtered_df = fixtures_df[fixtures_df['starting_at'] >= pd.Timestamp(\"2010-01-01\")]\n",
    "\n",
    "# Keep only valid fixture IDs\n",
    "valid_fixtures = filtered_df[['id']].dropna().astype(int)\n",
    "valid_fixtures.to_csv(\"/Users/sebastianvinther/Desktop/Sportsmonks/valid_fixtures_2010_onward.csv\", index=False)\n",
    "print(f\"Saved {len(valid_fixtures)} valid fixture IDs to valid_fixtures_2010_onward.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# === CONFIG ===\n",
    "API_TOKEN = 'oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd'  \n",
    "FIXTURE_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/valid_fixtures_2010_onward.csv'\n",
    "OUTPUT_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/fixture_statistics.csv'\n",
    "BATCH_SIZE = 100  # adjust if needed\n",
    "\n",
    "# === LOAD FIXTURE IDS ===\n",
    "fixture_df = pd.read_csv(FIXTURE_FILE)\n",
    "fixture_ids = fixture_df['id'].tolist()\n",
    "\n",
    "all_stats = []\n",
    "\n",
    "# === LOOP THROUGH FIXTURES ===\n",
    "for idx, fixture_id in enumerate(fixture_ids, 1):\n",
    "    url = f'https://api.sportmonks.com/v3/football/fixtures/{fixture_id}'\n",
    "    params = {\n",
    "        'api_token': API_TOKEN,\n",
    "        'include': 'statistics'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            stats = data.get('data', {}).get('statistics', [])\n",
    "            for stat in stats:\n",
    "                all_stats.append({\n",
    "                    'fixture_id': fixture_id,\n",
    "                    'type_id': stat.get('type_id'),\n",
    "                    'participant_id': stat.get('participant_id'),\n",
    "                    'value': stat.get('data', {}).get('value'),\n",
    "                    'location': stat.get('location')\n",
    "                })\n",
    "        else:\n",
    "            print(f\"[{fixture_id}] Error: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[{fixture_id}] Request failed: {e}\")\n",
    "\n",
    "    # === SAVE IN BATCHES ===\n",
    "    if idx % BATCH_SIZE == 0:\n",
    "        pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"[{idx}] Auto-saved {len(all_stats)} rows...\")\n",
    "\n",
    "    time.sleep(0.3)  # avoid hitting rate limits\n",
    "\n",
    "# === FINAL SAVE ===\n",
    "pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✅ Done! Total stats saved: {len(all_stats)} to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "API_TOKEN = 'oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd'  # Replace with your token\n",
    "FIXTURE_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/valid_fixtures_2010_onward.csv'\n",
    "OUTPUT_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/fixture_statistics.csv'\n",
    "BATCH_SIZE = 100  # adjust if needed\n",
    "\n",
    "# === LOAD FIXTURE IDS ===\n",
    "fixture_df = pd.read_csv(FIXTURE_FILE)\n",
    "fixture_ids = fixture_df['id'].tolist()\n",
    "\n",
    "all_stats = []\n",
    "\n",
    "# === LOOP WITH PROGRESS BAR ===\n",
    "print(f\"Starting download for {len(fixture_ids)} fixtures...\")\n",
    "for idx, fixture_id in enumerate(tqdm(fixture_ids, desc=\"Fetching stats\", unit=\"fixture\"), 1):\n",
    "    url = f'https://api.sportmonks.com/v3/football/fixtures/{fixture_id}'\n",
    "    params = {\n",
    "        'api_token': API_TOKEN,\n",
    "        'include': 'statistics'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            stats = data.get('data', {}).get('statistics', [])\n",
    "            for stat in stats:\n",
    "                all_stats.append({\n",
    "                    'fixture_id': fixture_id,\n",
    "                    'type_id': stat.get('type_id'),\n",
    "                    'participant_id': stat.get('participant_id'),\n",
    "                    'value': stat.get('data', {}).get('value'),\n",
    "                    'location': stat.get('location')\n",
    "                })\n",
    "        else:\n",
    "            print(f\"[{fixture_id}] Error: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[{fixture_id}] Request failed: {e}\")\n",
    "\n",
    "    # === SAVE IN BATCHES ===\n",
    "    if idx % BATCH_SIZE == 0:\n",
    "        pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"[{idx}] Auto-saved {len(all_stats)} rows...\")\n",
    "\n",
    "    \n",
    "\n",
    "# === FINAL SAVE ===\n",
    "pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✅ Done! Total stats saved: {len(all_stats)} to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "API_TOKEN = 'oYeoAVFUTQpu7MfoFqbvyiYfgRRkuBWW0p2atkZnySe4X3xrHkjgGhOvI0pd'  # Replace with your actual token\n",
    "FIXTURE_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/valid_fixtures_2010_onward.csv'\n",
    "OUTPUT_FILE = '/Users/sebastianvinther/Desktop/Sportsmonks/fixture_statistics.csv'\n",
    "BATCH_SIZE = 100  # Auto-save every 100 fixtures\n",
    "\n",
    "# === LOAD FIXTURE IDS ===\n",
    "fixture_df = pd.read_csv(FIXTURE_FILE)\n",
    "fixture_ids = fixture_df['id'].tolist()\n",
    "\n",
    "all_stats = []\n",
    "\n",
    "# === LOOP WITH PROGRESS BAR ===\n",
    "pbar = tqdm(total=len(fixture_ids), desc=\"Fetching Stats\", unit=\"fixture\")\n",
    "\n",
    "for idx, fixture_id in enumerate(fixture_ids, 1):\n",
    "    url = f'https://api.sportmonks.com/v3/football/fixtures/{fixture_id}'\n",
    "    params = {\n",
    "        'api_token': API_TOKEN,\n",
    "        'include': 'statistics'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            stats = data.get('data', {}).get('statistics', [])\n",
    "\n",
    "            for stat in stats:\n",
    "                flat_stat = {\n",
    "                    'fixture_id': fixture_id\n",
    "                }\n",
    "                for key, value in stat.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        for sub_key, sub_val in value.items():\n",
    "                            flat_stat[f\"{key}_{sub_key}\"] = sub_val\n",
    "                    else:\n",
    "                        flat_stat[key] = value\n",
    "                all_stats.append(flat_stat)\n",
    "\n",
    "        else:\n",
    "            print(f\"[{fixture_id}] Error: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[{fixture_id}] Request failed: {e}\")\n",
    "\n",
    "    # Auto-save in batches\n",
    "    if idx % BATCH_SIZE == 0:\n",
    "        pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"[{idx}] Auto-saved {len(all_stats)} rows...\")\n",
    "\n",
    "    # Update progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Final save\n",
    "pbar.close()\n",
    "pd.DataFrame(all_stats).to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✅ Done! Total stats saved: {len(all_stats)} to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the stats CSV\n",
    "df = pd.read_csv(\"/Users/sebastianvinther/Desktop/Sportsmonks/fixture_statistics.csv\")\n",
    "\n",
    "# Show unique stat type_ids and count how often each appears\n",
    "print(\"📊 Stat types (type_id counts):\")\n",
    "print(df['type_id'].value_counts())\n",
    "\n",
    "# See the number of unique type_ids\n",
    "print(f\"\\n🔢 Total unique stat types: {df['type_id'].nunique()}\")\n",
    "\n",
    "# Optional: Show recent fixture stat_ids to compare with older games\n",
    "latest_fixture_ids = df['fixture_id'].unique()[-10:]  # last 10 fixtures\n",
    "print(\"\\n🆕 Stat types from most recent 10 fixtures:\")\n",
    "print(df[df['fixture_id'].isin(latest_fixture_ids)]['type_id'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the stats data\n",
    "df = pd.read_csv('/Users/sebastianvinther/Desktop/Sportsmonks/fixture_statistics.csv')\n",
    "\n",
    "# Count unique type_ids\n",
    "unique_type_ids = df['type_id'].nunique()\n",
    "all_type_ids = df['type_id'].value_counts().sort_index()\n",
    "\n",
    "print(f\"🔢 Total unique type_ids: {unique_type_ids}\\n\")\n",
    "print(\"📊 type_id frequencies:\\n\")\n",
    "print(all_type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
